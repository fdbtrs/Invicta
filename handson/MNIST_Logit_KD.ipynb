{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5eTtD_OQvLUy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Caculate the number of trainable parameters\n",
    "def _calc_width(net):\n",
    "    import numpy as np\n",
    "    net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    weight_count = 0\n",
    "    for param in net_params:\n",
    "        weight_count += np.prod(param.size())\n",
    "    return weight_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Agd5cqiyeU3o",
    "outputId": "29efe6b1-bf48-4328-a6c4-39064232e99d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 12168456.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 337687.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 3050743.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4301316.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Define the Teacher Model\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the Student Model\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6070IhWmxOi_",
    "outputId": "be0a2550-8c77-4277-c57e-cd87b5372997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02288227342069149\n",
      "Epoch 2, Loss: 0.11291230469942093\n",
      "Epoch 3, Loss: 0.029640095308423042\n",
      "Epoch 4, Loss: 0.05165551230311394\n",
      "Epoch 5, Loss: 0.027807150036096573\n",
      "Epoch 6, Loss: 0.0002076236269203946\n",
      "Epoch 7, Loss: 0.010606239549815655\n",
      "Epoch 8, Loss: 2.8826119887526147e-05\n",
      "Epoch 9, Loss: 0.0003621918731369078\n",
      "Epoch 10, Loss: 9.19922676985152e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the Teacher Model\n",
    "teacher_model = TeacherModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_standalone(model):\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train_standalone(teacher_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFS_n9jjgdmS",
    "outputId": "5393f92e-ae2e-4165-824d-47ed1d7ffba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model Accuracy: 99.23%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate both models\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "teacher_acc = evaluate(teacher_model)\n",
    "print(f'Teacher Model Accuracy: {teacher_acc}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pQUj_Kf-gyzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.020250951871275902\n",
      "Epoch 2, Loss: 0.0027921730652451515\n",
      "Epoch 3, Loss: 0.008028319105505943\n",
      "Epoch 4, Loss: 0.02874193899333477\n",
      "Epoch 5, Loss: 5.718042666558176e-05\n",
      "Epoch 6, Loss: 0.0032480801455676556\n",
      "Epoch 7, Loss: 0.002733400324359536\n",
      "Epoch 8, Loss: 0.011748390272259712\n",
      "Epoch 9, Loss: 0.0014523735735565424\n",
      "Epoch 10, Loss: 2.4250031856354326e-05\n",
      "Standalone Student Model Accuracy: 98.93%\n",
      "Epoch 1, Loss: 0.14913146197795868\n",
      "Epoch 2, Loss: 0.12045740336179733\n",
      "Epoch 3, Loss: 0.2904188334941864\n",
      "Epoch 4, Loss: 0.13714228570461273\n",
      "Epoch 5, Loss: 0.06620895117521286\n",
      "Epoch 6, Loss: 0.04613114893436432\n",
      "Epoch 7, Loss: 0.059416841715574265\n",
      "Epoch 8, Loss: 0.060526445508003235\n",
      "Epoch 9, Loss: 0.098239466547966\n",
      "Epoch 10, Loss: 0.10634449124336243\n",
      "Student Model Accuracy: 99.03%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "standalone_student_model = StudentModel().cuda()\n",
    "optimizer = optim.Adam(standalone_student_model.parameters(), lr=0.001)\n",
    "\n",
    "train_standalone(standalone_student_model)\n",
    "standalone_student_acc = evaluate(standalone_student_model)\n",
    "print(f'Standalone Student Model Accuracy: {standalone_student_acc}%')\n",
    "\n",
    "# Knowledge Distillation Loss Function\n",
    "def kd_loss(student_logits, teacher_logits, labels, T=3, alpha=0.5):\n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(student_logits/T, dim=1),\n",
    "                                                  F.softmax(teacher_logits/T, dim=1)) * (T*T)\n",
    "    ce_loss = criterion(student_logits, labels)\n",
    "    return alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "# Train the Student Model using Knowledge Distillation\n",
    "student_model = StudentModel().cuda()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_student():\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student_model(images)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "            loss = kd_loss(student_outputs, teacher_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train_student()\n",
    "student_acc = evaluate(student_model)\n",
    "print(f'Student Model Accuracy: {student_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsLpbtAQwvea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
